{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hw-oh/wandb_e2e_demo/blob/main/models/image_classification/image_classification.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q wandb torchvision wandb-workspaces requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# === Colab Secretsì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ===\n",
        "# Colab ì¢Œì¸¡ ğŸ”‘ ì•„ì´ì½˜ â†’ ì•„ë˜ í‚¤ë“¤ì„ ë“±ë¡í•˜ì„¸ìš”\n",
        "WANDB_API_KEY = userdata.get(\"WANDB_API_KEY\")\n",
        "WANDB_ENTITY = userdata.get(\"WANDB_ENTITY\")       # W&B ì‚¬ìš©ìëª… ë˜ëŠ” íŒ€ëª…\n",
        "WANDB_PROJECT = userdata.get(\"WANDB_PROJECT\")       # ì˜ˆ: \"wandb-e2e-demo-image-classification\"\n",
        "WANDB_REGISTRY_NAME = userdata.get(\"WANDB_REGISTRY_NAME\")\n",
        "\n",
        "# Automations ì„¹ì…˜ìš© (ì„ íƒ â€” ìë™ ë°°í¬ íŒŒì´í”„ë¼ì¸ ì‚¬ìš© ì‹œ í•„ìš”)\n",
        "GITHUB_PAT = userdata.get(\"GITHUB_PAT\")             # GitHub Personal Access Token (repo scope)\n",
        "GITHUB_REPO = userdata.get(\"GITHUB_REPO\")            # ì˜ˆ: \"hw-oh/wandb_e2e_demo\"\n",
        "\n",
        "os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
        "\n",
        "wandb.login(key=WANDB_API_KEY)\n",
        "print(f\"Entity: {WANDB_ENTITY}\")\n",
        "print(f\"Project: {WANDB_PROJECT}\")\n",
        "print(f\"GitHub Repo: {GITHUB_REPO}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image Classification Demo â€” W&B ì „ì²´ ê¸°ëŠ¥ ì²´í—˜\n",
        "\n",
        "## ê°œìš”\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ **CIFAR-10** ë°ì´í„°ì…‹ê³¼ **ResNet-18** ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ë©´ì„œ,\n",
        "W&B(Weights & Biases)ì˜ í•µì‹¬ ê¸°ëŠ¥ì„ ì „ë¶€ ì²´í—˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "## ë‹¤ë£¨ëŠ” W&B ê¸°ëŠ¥\n",
        "\n",
        "| ê¸°ëŠ¥ | ì„¤ëª… |\n",
        "|------|------|\n",
        "| **Experiment Tracking** | í•™ìŠµ ë©”íŠ¸ë¦­ ì‹¤ì‹œê°„ ì¶”ì  (`wandb.init`, `wandb.log`, `wandb.config`) |\n",
        "| **Media Logging** | ì´ë¯¸ì§€ ë¡œê¹… (`wandb.Image`) |\n",
        "| **Tables** | ë°ì´í„°ì…‹ ë¯¸ë¦¬ë³´ê¸° ë° ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ (`wandb.Table`) |\n",
        "| **Artifacts** | ë°ì´í„°ì…‹/ëª¨ë¸ ë²„ì €ë‹ ë° ê³„ë³´(lineage) ì¶”ì  |\n",
        "| **Model Registry** | ëª¨ë¸ ë“±ë¡ ë° alias ê´€ë¦¬ (staging/production) |\n",
        "| **Sweeps** | ë² ì´ì§€ì•ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” |\n",
        "| **Reports** | í”„ë¡œê·¸ë˜ë° ë°©ì‹ ì‹¤í—˜ ë¦¬í¬íŠ¸ ìƒì„± |\n",
        "| **Automations** | Model Registry ìŠ¹ê²© â†’ Webhook â†’ GitHub Actions ìë™ ë°°í¬ |\n",
        "\n",
        "## ë°ì´í„°ì…‹\n",
        "- **CIFAR-10**: 60,000ì¥ (32Ã—32 RGB), 10ê°œ í´ë˜ìŠ¤\n",
        "- torchvision ë‚´ì¥ ë°ì´í„°ì…‹ìœ¼ë¡œ ë³„ë„ ë‹¤ìš´ë¡œë“œ ë¶ˆí•„ìš”\n",
        "\n",
        "## ëª¨ë¸\n",
        "- **ResNet-18** (ImageNet pretrained â†’ CIFAR-10 fine-tune)\n",
        "- 32Ã—32 ì…ë ¥ì— ë§ê²Œ conv1 ë° maxpool ìˆ˜ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "import random\n",
        "import time\n",
        "\n",
        "# === í•™ìŠµ ì„¤ì • ===\n",
        "CONFIG = {\n",
        "    \"batch_size\": 64,\n",
        "    \"lr\": 1e-3,\n",
        "    \"epochs\": 5,\n",
        "    \"optimizer\": \"adam\",\n",
        "    \"num_classes\": 10,\n",
        "    \"img_size\": 32,\n",
        "    \"model_name\": \"resnet18\",\n",
        "    \"dataset\": \"cifar10\",\n",
        "}\n",
        "\n",
        "CIFAR10_CLASSES = [\n",
        "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
        "]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === ë°ì´í„° ë¡œë“œ + Transform + DataLoader ===\n",
        "\n",
        "# CIFAR-10 ì „ìš© ì •ê·œí™” ê°’ (ì´ë¯¸ì§€ë„· ê°’ê³¼ ë‹¤ë¦„)\n",
        "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
        "CIFAR10_STD = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=transform_train\n",
        ")\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=transform_test\n",
        ")\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=2\n",
        ")\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=2\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(trainset)}ì¥, Test: {len(testset)}ì¥\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === ë°ì´í„°ì…‹ Artifact ìƒì„± ===\n",
        "\n",
        "run = wandb.init(\n",
        "    entity=WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    config=CONFIG,\n",
        "    job_type=\"data-versioning\",\n",
        "    name=\"cifar10-data-versioning\",\n",
        ")\n",
        "\n",
        "artifact = wandb.Artifact(\n",
        "    \"cifar10\",\n",
        "    type=\"dataset\",\n",
        "    description=\"CIFAR-10 dataset (torchvision)\",\n",
        "    metadata={\n",
        "        \"num_train\": len(trainset),\n",
        "        \"num_test\": len(testset),\n",
        "        \"num_classes\": 10,\n",
        "        \"image_size\": \"32x32\",\n",
        "        \"classes\": CIFAR10_CLASSES,\n",
        "        \"source\": \"torchvision.datasets.CIFAR10\",\n",
        "    },\n",
        ")\n",
        "artifact.add_dir(\"./data/cifar-10-batches-py\")\n",
        "run.log_artifact(artifact)\n",
        "print(\"ë°ì´í„°ì…‹ Artifact ë¡œê¹… ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === ìƒ˜í”Œ ì´ë¯¸ì§€ wandb.Table ì‹œê°í™” ===\n",
        "\n",
        "# ì‹œê°í™”ìš©ìœ¼ë¡œ ì •ê·œí™” ì•ˆ ëœ ì›ë³¸ ë°ì´í„° ë¡œë“œ\n",
        "raw_dataset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=False, transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "# í´ë˜ìŠ¤ë³„ 5ì¥ = ì´ 50ì¥\n",
        "table = wandb.Table(columns=[\"Image\", \"Label\", \"Label_ID\"])\n",
        "\n",
        "class_indices = {i: [] for i in range(10)}\n",
        "for idx, (_, label) in enumerate(raw_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "for class_id in range(10):\n",
        "    samples = random.sample(class_indices[class_id], 5)\n",
        "    for idx in samples:\n",
        "        img, label = raw_dataset[idx]\n",
        "        table.add_data(wandb.Image(img), CIFAR10_CLASSES[label], label)\n",
        "\n",
        "wandb.log({\"dataset_preview\": table})\n",
        "wandb.finish()\n",
        "print(\"ë°ì´í„°ì…‹ ë¯¸ë¦¬ë³´ê¸° í…Œì´ë¸” ë¡œê¹… ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === ëª¨ë¸ ì •ì˜ (ResNet-18, CIFAR-10ìš© ìˆ˜ì •) ===\n",
        "\n",
        "def create_model(num_classes=10):\n",
        "    \"\"\"\n",
        "    CIFAR-10(32x32) ì…ë ¥ì— ë§ê²Œ ìˆ˜ì •í•œ ResNet-18.\n",
        "    \n",
        "    í‘œì¤€ ResNet-18ì€ 224x224ìš©ìœ¼ë¡œ conv1(7x7, stride=2) + maxpool(3x3, stride=2)ê°€\n",
        "    32x32 ì…ë ¥ì„ 1x1ë¡œ ì¶•ì†Œì‹œì¼œ í•™ìŠµì´ ë¶ˆê°€ëŠ¥í•´ì§.\n",
        "    ìˆ˜ì •: conv1 -> 3x3, stride=1, padding=1 / maxpool -> Identity\n",
        "    \"\"\"\n",
        "    model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    model.maxpool = nn.Identity()\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "model = create_model(CONFIG[\"num_classes\"]).to(device)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === í•™ìŠµ ë£¨í”„ ===\n",
        "\n",
        "run = wandb.init(\n",
        "    entity=WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    config=CONFIG,\n",
        "    job_type=\"training\",\n",
        "    name=\"resnet18-cifar10-baseline\",\n",
        ")\n",
        "\n",
        "# ë°ì´í„° Artifactë¥¼ ì…ë ¥ìœ¼ë¡œ ì„ ì–¸ â†’ lineage ì—°ê²°\n",
        "# (ë°ì´í„° Artifact â†’ í•™ìŠµ Run â†’ ëª¨ë¸ Artifact)\n",
        "data_artifact = run.use_artifact(f\"{WANDB_ENTITY}/{WANDB_PROJECT}/cifar10:latest\")\n",
        "print(f\"ë°ì´í„° Artifact ì‚¬ìš©: {data_artifact.name}:{data_artifact.version}\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "if CONFIG[\"optimizer\"] == \"adam\":\n",
        "    optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "elif CONFIG[\"optimizer\"] == \"sgd\":\n",
        "    optimizer = optim.SGD(model.parameters(), lr=CONFIG[\"lr\"], momentum=0.9, weight_decay=5e-4)\n",
        "elif CONFIG[\"optimizer\"] == \"adamw\":\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG[\"lr\"], weight_decay=1e-2)\n",
        "\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG[\"epochs\"])\n",
        "\n",
        "# ëª¨ë¸ gradient/parameter ë¡œê¹…\n",
        "wandb.watch(model, criterion, log=\"all\", log_freq=100)\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            wandb.log({\n",
        "                \"train/batch_loss\": loss.item(),\n",
        "                \"train/batch_acc\": 100.0 * predicted.eq(targets).sum().item() / targets.size(0),\n",
        "            })\n",
        "\n",
        "    return running_loss / len(loader), 100.0 * correct / total\n",
        "\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return running_loss / len(loader), 100.0 * correct / total\n",
        "\n",
        "\n",
        "# ë©”ì¸ í•™ìŠµ ë£¨í”„\n",
        "best_acc = 0.0\n",
        "\n",
        "for epoch in range(CONFIG[\"epochs\"]):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train_one_epoch(model, trainloader, criterion, optimizer)\n",
        "    val_loss, val_acc = evaluate(model, testloader, criterion)\n",
        "    scheduler.step()\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    # Epoch ë ˆë²¨ ë©”íŠ¸ë¦­ ë¡œê¹…\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train/loss\": train_loss,\n",
        "        \"train/acc\": train_acc,\n",
        "        \"val/loss\": val_loss,\n",
        "        \"val/acc\": val_acc,\n",
        "        \"lr\": scheduler.get_last_lr()[0],\n",
        "        \"epoch_time_sec\": epoch_time,\n",
        "    })\n",
        "\n",
        "    # ì˜ˆì¸¡ ì´ë¯¸ì§€ ë¡œê¹… (denormalize í•„ìˆ˜)\n",
        "    model.eval()\n",
        "    images, labels = next(iter(testloader))\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images.to(device))\n",
        "        _, preds = outputs.max(1)\n",
        "        preds = preds.cpu()\n",
        "\n",
        "    mean = torch.tensor(CIFAR10_MEAN).view(3, 1, 1)\n",
        "    std = torch.tensor(CIFAR10_STD).view(3, 1, 1)\n",
        "    wandb_images = []\n",
        "    for i in range(8):\n",
        "        img = images[i] * std + mean  # denormalize\n",
        "        img = img.clamp(0, 1)\n",
        "        caption = f\"True: {CIFAR10_CLASSES[labels[i]]} | Pred: {CIFAR10_CLASSES[preds[i]]}\"\n",
        "        wandb_images.append(wandb.Image(img, caption=caption))\n",
        "    wandb.log({\"predictions\": wandb_images})\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "    print(\n",
        "        f\"Epoch [{epoch+1}/{CONFIG['epochs']}] \"\n",
        "        f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | \"\n",
        "        f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}%\"\n",
        "    )\n",
        "\n",
        "wandb.summary[\"best_val_acc\"] = best_acc\n",
        "print(f\"\\ní•™ìŠµ ì™„ë£Œ! Best Val Acc: {best_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === ê²€ì¦ ê²°ê³¼ wandb.Table ===\n",
        "\n",
        "model.load_state_dict(torch.load(\"best_model.pth\", map_location=device, weights_only=True))\n",
        "model.eval()\n",
        "\n",
        "# ì‹œê°í™”ìš© ì›ë³¸ ë°ì´í„°\n",
        "raw_testset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=False, transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "columns = [\"Image\", \"True Label\", \"Predicted Label\", \"Correct\", \"Confidence\"]\n",
        "for cls in CIFAR10_CLASSES:\n",
        "    columns.append(f\"P({cls})\")\n",
        "\n",
        "results_table = wandb.Table(columns=columns)\n",
        "num_samples = 200\n",
        "indices = random.sample(range(len(testset)), num_samples)\n",
        "\n",
        "for idx in indices:\n",
        "    img_raw, label = raw_testset[idx]\n",
        "    img_norm, _ = testset[idx]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(img_norm.unsqueeze(0).to(device))\n",
        "        probs = torch.softmax(output, dim=1)[0].cpu()\n",
        "        pred = probs.argmax().item()\n",
        "        confidence = probs[pred].item()\n",
        "\n",
        "    row = [\n",
        "        wandb.Image(img_raw),\n",
        "        CIFAR10_CLASSES[label],\n",
        "        CIFAR10_CLASSES[pred],\n",
        "        label == pred,\n",
        "        round(confidence, 4),\n",
        "    ]\n",
        "    for p in probs.tolist():\n",
        "        row.append(round(p, 4))\n",
        "\n",
        "    results_table.add_data(*row)\n",
        "\n",
        "wandb.log({\"test_predictions\": results_table})\n",
        "print(f\"í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ê²°ê³¼ {num_samples}ê±´ ë¡œê¹… ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === ëª¨ë¸ Artifact ì €ì¥ ===\n",
        "\n",
        "model_artifact = wandb.Artifact(\n",
        "    \"resnet18-cifar10\",\n",
        "    type=\"model\",\n",
        "    description=\"ResNet-18 fine-tuned on CIFAR-10\",\n",
        "    metadata={\n",
        "        \"model_type\": \"classification\",\n",
        "        \"model_architecture\": \"resnet18\",\n",
        "        \"dataset\": \"cifar10\",\n",
        "        \"num_classes\": 10,\n",
        "        \"best_val_acc\": best_acc,\n",
        "        \"classes\": CIFAR10_CLASSES,\n",
        "        \"framework\": \"pytorch\",\n",
        "        \"input_size\": [3, 32, 32],\n",
        "    },\n",
        ")\n",
        "model_artifact.add_file(\"best_model.pth\", name=\"model.pth\")\n",
        "run.log_artifact(model_artifact)\n",
        "print(\"ëª¨ë¸ Artifact ë¡œê¹… ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Model Registry ë“±ë¡ ===\n",
        "\n",
        "run.link_artifact(\n",
        "    model_artifact,\n",
        "    f\"{WANDB_REGISTRY_NAME}/cifar10-classifier\",\n",
        "    aliases=[\"staging\"],\n",
        ")\n",
        "print(\"Model Registryì— 'staging' aliasë¡œ ë“±ë¡ ì™„ë£Œ!\")\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter Sweep\n",
        "\n",
        "**Bayesian ìµœì í™”**ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì„ íƒìƒ‰í•©ë‹ˆë‹¤.\n",
        "\n",
        "| íŒŒë¼ë¯¸í„° | íƒìƒ‰ ë²”ìœ„ |\n",
        "|-----------|------------|\n",
        "| Learning Rate | 1e-5 ~ 1e-2 (log uniform) |\n",
        "| Batch Size | 32, 64, 128 |\n",
        "| Optimizer | Adam, SGD, AdamW |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Sweep ì„¤ì • ë° ì‹¤í–‰ ===\n",
        "\n",
        "sweep_config = {\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\"name\": \"val/acc\", \"goal\": \"maximize\"},\n",
        "    \"parameters\": {\n",
        "        \"lr\": {\"min\": 1e-5, \"max\": 1e-2, \"distribution\": \"log_uniform_values\"},\n",
        "        \"batch_size\": {\"values\": [32, 64, 128]},\n",
        "        \"optimizer\": {\"values\": [\"adam\", \"sgd\", \"adamw\"]},\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "def sweep_train():\n",
        "    \"\"\"Sweep í•™ìŠµ í•¨ìˆ˜ (ì¸ì ì—†ìŒ â€” wandb.agent ê·œì¹™)\"\"\"\n",
        "    run = wandb.init(entity=WANDB_ENTITY, config=CONFIG)\n",
        "    config = wandb.config\n",
        "\n",
        "    # Sweep íŒŒë¼ë¯¸í„°ë¡œ DataLoader ì¬ìƒì„±\n",
        "    sweep_trainloader = torch.utils.data.DataLoader(\n",
        "        trainset, batch_size=config.batch_size, shuffle=True, num_workers=2\n",
        "    )\n",
        "    sweep_testloader = torch.utils.data.DataLoader(\n",
        "        testset, batch_size=config.batch_size, shuffle=False, num_workers=2\n",
        "    )\n",
        "\n",
        "    # ê° trialë§ˆë‹¤ ìƒˆ ëª¨ë¸\n",
        "    sweep_model = create_model(CONFIG[\"num_classes\"]).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if config.optimizer == \"adam\":\n",
        "        opt = optim.Adam(sweep_model.parameters(), lr=config.lr)\n",
        "    elif config.optimizer == \"sgd\":\n",
        "        opt = optim.SGD(sweep_model.parameters(), lr=config.lr, momentum=0.9, weight_decay=5e-4)\n",
        "    elif config.optimizer == \"adamw\":\n",
        "        opt = optim.AdamW(sweep_model.parameters(), lr=config.lr, weight_decay=1e-2)\n",
        "\n",
        "    # Sweepì—ì„œëŠ” 3 epochìœ¼ë¡œ ì œí•œ (ë°ëª¨ ì‹œê°„ ì ˆì•½)\n",
        "    for epoch in range(3):\n",
        "        train_loss, train_acc = train_one_epoch(sweep_model, sweep_trainloader, criterion, opt)\n",
        "        val_loss, val_acc = evaluate(sweep_model, sweep_testloader, criterion)\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train/loss\": train_loss,\n",
        "            \"train/acc\": train_acc,\n",
        "            \"val/loss\": val_loss,\n",
        "            \"val/acc\": val_acc,\n",
        "        })\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, entity=WANDB_ENTITY, project=WANDB_PROJECT)\n",
        "wandb.agent(sweep_id, function=sweep_train, count=5)\n",
        "print(\"Sweep ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Report ìƒì„± ===\n",
        "\n",
        "import wandb_workspaces.reports.v2 as wr\n",
        "\n",
        "report = wr.Report(\n",
        "    entity=WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    title=\"CIFAR-10 Image Classification â€” ì‹¤í—˜ ê²°ê³¼ ë¦¬í¬íŠ¸\",\n",
        "    description=\"ResNet-18 CIFAR-10 fine-tuning ì‹¤í—˜ ê²°ê³¼ ë° Sweep ë¶„ì„\",\n",
        ")\n",
        "\n",
        "report.blocks = [\n",
        "    wr.TableOfContents(),\n",
        "\n",
        "    wr.H1(\"1. ì‹¤í—˜ ê°œìš”\"),\n",
        "    wr.P(\n",
        "        \"CIFAR-10 ë°ì´í„°ì…‹ì— ëŒ€í•œ ResNet-18 ì´ë¯¸ì§€ ë¶„ë¥˜ ì‹¤í—˜ ê²°ê³¼ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤. \"\n",
        "        \"W&Bì˜ Experiment Tracking, Artifacts, Sweeps, Model Registry ê¸°ëŠ¥ì„ í™œìš©í•˜ì˜€ìŠµë‹ˆë‹¤.\"\n",
        "    ),\n",
        "\n",
        "    wr.H1(\"2. í•™ìŠµ ê²°ê³¼\"),\n",
        "    wr.PanelGrid(\n",
        "        runsets=[\n",
        "            wr.Runset(entity=WANDB_ENTITY, project=WANDB_PROJECT)\n",
        "        ],\n",
        "        panels=[\n",
        "            wr.LinePlot(title=\"Training Loss\", x=\"epoch\", y=[\"train/loss\"]),\n",
        "            wr.LinePlot(title=\"Validation Accuracy\", x=\"epoch\", y=[\"val/acc\"]),\n",
        "            wr.LinePlot(title=\"Validation Loss\", x=\"epoch\", y=[\"val/loss\"]),\n",
        "            wr.LinePlot(title=\"Training Accuracy\", x=\"epoch\", y=[\"train/acc\"]),\n",
        "        ],\n",
        "    ),\n",
        "\n",
        "    wr.H1(\"3. Sweep ë¶„ì„\"),\n",
        "    wr.P(\"Bayesian ìµœì í™”ë¥¼ í†µí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ê²°ê³¼:\"),\n",
        "    wr.PanelGrid(\n",
        "        runsets=[\n",
        "            wr.Runset(entity=WANDB_ENTITY, project=WANDB_PROJECT)\n",
        "        ],\n",
        "        panels=[\n",
        "            wr.ParallelCoordinatesPlot(\n",
        "                columns=[\n",
        "                    wr.ParallelCoordinatesPlotColumn(metric=\"c::lr\"),\n",
        "                    wr.ParallelCoordinatesPlotColumn(metric=\"c::batch_size\"),\n",
        "                    wr.ParallelCoordinatesPlotColumn(metric=\"c::optimizer\"),\n",
        "                    wr.ParallelCoordinatesPlotColumn(metric=\"val/acc\"),\n",
        "                ],\n",
        "            ),\n",
        "            wr.ScalarChart(title=\"Best Validation Accuracy\", metric=\"val/acc\"),\n",
        "            wr.BarPlot(title=\"Val Accuracy by Run\", metrics=[\"val/acc\"]),\n",
        "        ],\n",
        "    ),\n",
        "\n",
        "    wr.H1(\"4. ë‹¤ìŒ ë‹¨ê³„\"),\n",
        "    wr.P(\n",
        "        \"ìµœì  ëª¨ë¸ì„ Model Registryì˜ 'production' aliasë¡œ ìŠ¹ê²©í•˜ì—¬ \"\n",
        "        \"ë°°í¬ íŒŒì´í”„ë¼ì¸ì„ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "report.save()\n",
        "print(f\"Report ìƒì„± ì™„ë£Œ! URL: {report.url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.finish()\n",
        "print(\"\\ní•™ìŠµ & ì‹¤í—˜ ì¶”ì  ì™„ë£Œ!\")\n",
        "print(\"W&B ëŒ€ì‹œë³´ë“œì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "print(\"\\nì•„ë˜ ì„¹ì…˜ì—ì„œ ëª¨ë¸ì„ 'production'ìœ¼ë¡œ ìŠ¹ê²©í•˜ì—¬ ìë™ ë°°í¬ íŒŒì´í”„ë¼ì¸ì„ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# W&B Automations â€” ìë™ ë°°í¬ íŒŒì´í”„ë¼ì¸\n",
        "\n",
        "## ì „ì²´ ì•„í‚¤í…ì²˜\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  ì´ ë…¸íŠ¸ë¶   â”‚     â”‚  W&B Model       â”‚     â”‚  GitHub Actions     â”‚     â”‚  Streamlit       â”‚\n",
        "â”‚  (Colab)    â”‚â”€â”€â”€â–¶â”‚  Registry        â”‚â”€â”€â”€â–¶â”‚  Workflow           â”‚â”€â”€â”€â–¶â”‚  Cloud           â”‚\n",
        "â”‚             â”‚     â”‚                  â”‚     â”‚                     â”‚     â”‚                  â”‚\n",
        "â”‚ ëª¨ë¸ ìŠ¹ê²©    â”‚     â”‚ \"production\"     â”‚     â”‚ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ       â”‚     â”‚ ìƒˆ ëª¨ë¸ë¡œ ì„œë¹™   â”‚\n",
        "â”‚ ì‹¤í–‰        â”‚     â”‚ alias ìŠ¹ê²© ì‹œ    â”‚     â”‚ â†’ ì•± ì¬ë°°í¬         â”‚     â”‚ â†’ ì¶”ë¡  í…ŒìŠ¤íŠ¸    â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### íë¦„\n",
        "1. ìœ„ í•™ìŠµ ê³¼ì •ì—ì„œ ëª¨ë¸ì´ Model Registryì— **\"staging\"** ìœ¼ë¡œ ë“±ë¡ë¨\n",
        "2. ì•„ë˜ ì…€ì—ì„œ í•´ë‹¹ ëª¨ë¸ì„ **\"production\"** ìœ¼ë¡œ ìŠ¹ê²©\n",
        "3. W&B Automationì´ Webhook ë°œë™ â†’ GitHub Actions `repository_dispatch`\n",
        "4. GitHub Actionsê°€ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ + `deployment.json` ì—…ë°ì´íŠ¸ + commit & push\n",
        "5. Streamlit Cloudê°€ ìë™ ì¬ë°°í¬ â†’ ìƒˆ ëª¨ë¸ë¡œ ì„œë¹™\n",
        "\n",
        "## ì‚¬ì „ ì¤€ë¹„\n",
        "\n",
        "| í•­ëª© | ì„¤ëª… |\n",
        "|------|------|\n",
        "| **GITHUB_PAT** | GitHub Settings â†’ Developer settings â†’ Personal access tokens â†’ `repo` scope |\n",
        "| **GITHUB_REPO** | Colab Secretsì— ë“±ë¡ (ì˜ˆ: `hw-oh/wandb_e2e_demo`) |\n",
        "| **W&B Webhook** | W&B Settings â†’ Webhooks â†’ GitHub `repository_dispatch` URL ë“±ë¡ |\n",
        "| **W&B Automation** | Automations â†’ alias = `production` ì¶”ê°€ ì‹œ Webhook íŠ¸ë¦¬ê±° |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Registry í˜„í™© ì¡°íšŒ ===\n",
        "\n",
        "REGISTERED_MODEL = \"cifar10-classifier\"\n",
        "\n",
        "api = wandb.Api()\n",
        "\n",
        "collections = api.artifact_type(\"model\", project=f\"{WANDB_ENTITY}/{WANDB_PROJECT}\").collections()\n",
        "\n",
        "for collection in collections:\n",
        "    print(f\"\\nModel: {collection.name}\")\n",
        "    print(\"-\" * 50)\n",
        "    for version in collection.versions():\n",
        "        aliases = \", \".join(version.aliases) if version.aliases else \"-\"\n",
        "        print(f\"  {version.version} | aliases: [{aliases}] | created: {version.created_at}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === \"production\" ìŠ¹ê²© ì‹¤í–‰ ===\n",
        "# ìŠ¹ê²©í•  ë²„ì „ì„ ì„ íƒí•˜ì„¸ìš” (ì˜ˆ: \"latest\" ë˜ëŠ” íŠ¹ì • ë²„ì „ \"v0\")\n",
        "\n",
        "PROMOTE_VERSION = \"latest\"  # ë˜ëŠ” \"v0\", \"v1\" ë“± íŠ¹ì • ë²„ì „\n",
        "\n",
        "artifact = api.artifact(\n",
        "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/{REGISTERED_MODEL}:{PROMOTE_VERSION}\"\n",
        ")\n",
        "\n",
        "if \"production\" in artifact.aliases:\n",
        "    print(f\"{artifact.version}ì€ ì´ë¯¸ productionì…ë‹ˆë‹¤.\")\n",
        "else:\n",
        "    artifact.aliases.append(\"production\")\n",
        "    artifact.save()\n",
        "    print(f\"{artifact.name}:{artifact.version} â†’ production ìŠ¹ê²© ì™„ë£Œ!\")\n",
        "    print(\"\\nW&B Automationì´ GitHub Actions Webhookì„ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤...\")\n",
        "    print(\"ë°°í¬ íŒŒì´í”„ë¼ì¸ì´ ìë™ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === GitHub Actions ì‹¤í–‰ ìƒíƒœ í™•ì¸ ===\n",
        "\n",
        "import requests\n",
        "import time as _time\n",
        "\n",
        "print(\"GitHub Actions ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ëŒ€ê¸° ì¤‘... (30ì´ˆ)\")\n",
        "_time.sleep(30)  # Webhook ì „íŒŒ + ì›Œí¬í”Œë¡œìš° ì‹œì‘ ëŒ€ê¸°\n",
        "\n",
        "resp = requests.get(\n",
        "    f\"https://api.github.com/repos/{GITHUB_REPO}/actions/runs\",\n",
        "    headers={\"Authorization\": f\"Bearer {GITHUB_PAT}\"},\n",
        "    params={\"event\": \"repository_dispatch\", \"per_page\": 5},\n",
        ")\n",
        "\n",
        "if resp.status_code == 200:\n",
        "    runs = resp.json().get(\"workflow_runs\", [])\n",
        "    if runs:\n",
        "        print(\"\\nìµœê·¼ ë°°í¬ ì›Œí¬í”Œë¡œìš°:\")\n",
        "        for r in runs[:3]:\n",
        "            status_map = {\"completed\": \"ì™„ë£Œ\", \"in_progress\": \"ì‹¤í–‰ì¤‘\", \"queued\": \"ëŒ€ê¸°ì¤‘\"}\n",
        "            status = status_map.get(r[\"status\"], r[\"status\"])\n",
        "            print(f\"  [{status}] {r['name']} - {r['created_at']}\")\n",
        "            print(f\"    URL: {r['html_url']}\")\n",
        "    else:\n",
        "        print(\"repository_dispatch ì›Œí¬í”Œë¡œìš°ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "else:\n",
        "    print(f\"GitHub API ì˜¤ë¥˜: {resp.status_code}\")\n",
        "    print(resp.json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === ë°°í¬ ìƒíƒœ í™•ì¸ ===\n",
        "\n",
        "import json as _json\n",
        "import base64\n",
        "\n",
        "resp = requests.get(\n",
        "    f\"https://api.github.com/repos/{GITHUB_REPO}/contents/models/app/deployment.json\",\n",
        "    headers={\"Authorization\": f\"Bearer {GITHUB_PAT}\"},\n",
        ")\n",
        "\n",
        "if resp.status_code == 200:\n",
        "    content = base64.b64decode(resp.json()[\"content\"]).decode(\"utf-8\")\n",
        "    deploy_info = _json.loads(content)\n",
        "    print(\"\\ní˜„ì¬ ë°°í¬ ìƒíƒœ:\")\n",
        "    print(f\"  ëª¨ë¸: {deploy_info['model_name']}\")\n",
        "    print(f\"  ë²„ì „: {deploy_info['model_version']}\")\n",
        "    print(f\"  ë°°í¬ ì‹œê°: {deploy_info['deployed_at']}\")\n",
        "else:\n",
        "    print(f\"deployment.json ì¡°íšŒ ì‹¤íŒ¨: {resp.status_code}\")\n",
        "    print(\"(ì•„ì§ ë°°í¬ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ë³´ì„¸ìš”.)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ë¡¤ë°± ê°€ì´ë“œ\n",
        "\n",
        "ë°°í¬ëœ ëª¨ë¸ì— ë¬¸ì œê°€ ë°œìƒí•˜ë©´, ì´ì „ ë²„ì „ì„ ë‹¤ì‹œ \"production\"ìœ¼ë¡œ ìŠ¹ê²©í•˜ì—¬ ë¡¤ë°±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "### ë¡¤ë°± ì ˆì°¨\n",
        "1. í˜„ì¬ production ëª¨ë¸ì˜ ë¬¸ì œ í™•ì¸\n",
        "2. ì´ì „ ì•ˆì • ë²„ì „ ì‹ë³„ (ìœ„ Registry í˜„í™© ì°¸ì¡°)\n",
        "3. ì´ì „ ë²„ì „ì— \"production\" alias ì´ë™\n",
        "4. ìë™ìœ¼ë¡œ ë°°í¬ íŒŒì´í”„ë¼ì¸ ì¬ì‹¤í–‰\n",
        "\n",
        "> **ì°¸ê³ **: W&B Model RegistryëŠ” alias ì´ë™ ì‹œì—ë„ Automationì„ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤.\n",
        "> ë¡¤ë°±ë„ ìŠ¹ê²©ê³¼ ë™ì¼í•œ íŒŒì´í”„ë¼ì¸ì„ íƒ€ë¯€ë¡œ ë³„ë„ ì¡°ì¹˜ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === ë¡¤ë°± ì‹¤í–‰ ì˜ˆì‹œ ===\n",
        "# ì´ì „ ë²„ì „ìœ¼ë¡œ ë¡¤ë°±í•˜ë ¤ë©´ ì•„ë˜ ì½”ë“œì˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ë²„ì „ì„ ìˆ˜ì •í•˜ì„¸ìš”\n",
        "\n",
        "# ROLLBACK_VERSION = \"v0\"  # ë¡¤ë°±í•  ë²„ì „\n",
        "#\n",
        "# rollback_artifact = api.artifact(\n",
        "#     f\"{WANDB_ENTITY}/{WANDB_PROJECT}/{REGISTERED_MODEL}:{ROLLBACK_VERSION}\"\n",
        "# )\n",
        "#\n",
        "# # í˜„ì¬ productionì—ì„œ alias ì œê±°\n",
        "# current_prod = api.artifact(\n",
        "#     f\"{WANDB_ENTITY}/{WANDB_PROJECT}/{REGISTERED_MODEL}:production\"\n",
        "# )\n",
        "# current_prod.aliases.remove(\"production\")\n",
        "# current_prod.save()\n",
        "#\n",
        "# # ì´ì „ ë²„ì „ì— production alias ì¶”ê°€\n",
        "# rollback_artifact.aliases.append(\"production\")\n",
        "# rollback_artifact.save()\n",
        "# print(f\"ë¡¤ë°± ì™„ë£Œ! {ROLLBACK_VERSION} â†’ production\")\n",
        "\n",
        "print(\"ë¡¤ë°± ì˜ˆì‹œ ì½”ë“œì…ë‹ˆë‹¤. í•„ìš” ì‹œ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
