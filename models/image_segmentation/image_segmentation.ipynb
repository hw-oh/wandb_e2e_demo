{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESis4qyAA8Pc"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hw-oh/wandb_e2e_demo/blob/main/models/image_segmentation/image_segmentation.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vkhru_NvA8Pd",
        "outputId": "ec68ba11-be4b-42da-fb2e-48df5f451906"
      },
      "source": [
        "!pip install -q wandb torchvision wandb-workspaces"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/96.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.3/96.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hc-YyGDFA8Pe",
        "outputId": "e43009ff-d892-4721-8df8-bda1f9ca802a"
      },
      "source": [
        "import wandb\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# === Colab Secretsì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ===\n",
        "# Colab ì¢Œì¸¡ ğŸ”‘ ì•„ì´ì½˜ â†’ ì•„ë˜ í‚¤ë“¤ì„ ë“±ë¡í•˜ì„¸ìš”\n",
        "WANDB_API_KEY = userdata.get(\"WANDB_API_KEY\")\n",
        "WANDB_ENTITY = userdata.get(\"WANDB_ENTITY\")\n",
        "WANDB_PROJECT = userdata.get(\"WANDB_PROJECT\")\n",
        "WANDB_REGISTRY_NAME = userdata.get(\"WANDB_REGISTRY_NAME\")\n",
        "\n",
        "os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
        "\n",
        "wandb.login(key=WANDB_API_KEY)\n",
        "print(f\"Entity: {WANDB_ENTITY}\")\n",
        "print(f\"Project: {WANDB_PROJECT}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Using explicit session credentials for https://api.wandb.ai.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhyunwoo-oh\u001b[0m (\u001b[33mwandb-korea\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: wandb-korea\n",
            "Project: e2e-demo-image-classification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHxhvPD7A8Pe"
      },
      "source": [
        "# Image Segmentation Demo â€” W&B ì „ì²´ ê¸°ëŠ¥ ì²´í—˜\n",
        "\n",
        "## ê°œìš”\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ **Oxford-IIIT Pet Dataset**ê³¼ **DeepLabV3 (MobileNetV3-Large backbone)** ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬\n",
        "ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ìˆ˜í–‰í•˜ë©´ì„œ, W&B(Weights & Biases)ì˜ í•µì‹¬ ê¸°ëŠ¥ì„ ì „ë¶€ ì²´í—˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "## ë‹¤ë£¨ëŠ” W&B ê¸°ëŠ¥\n",
        "\n",
        "| ê¸°ëŠ¥ | ì„¤ëª… |\n",
        "|------|------|\n",
        "| **Experiment Tracking** | í•™ìŠµ ë©”íŠ¸ë¦­ ì‹¤ì‹œê°„ ì¶”ì  (`wandb.init`, `wandb.log`, `wandb.config`) |\n",
        "| **Media Logging** | ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´ (`wandb.Image` + `masks` íŒŒë¼ë¯¸í„°) |\n",
        "| **Tables** | ë°ì´í„°ì…‹ ë¯¸ë¦¬ë³´ê¸° ë° ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ (`wandb.Table`) |\n",
        "| **Artifacts** | ë°ì´í„°ì…‹/ëª¨ë¸ ë²„ì €ë‹ ë° ê³„ë³´(lineage) ì¶”ì  |\n",
        "| **Model Registry** | ëª¨ë¸ ë“±ë¡ ë° alias ê´€ë¦¬ (staging) |\n",
        "| **Sweeps** | ë² ì´ì§€ì•ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” |\n",
        "| **Reports** | í”„ë¡œê·¸ë˜ë° ë°©ì‹ ì‹¤í—˜ ë¦¬í¬íŠ¸ ìƒì„± |\n",
        "\n",
        "## ë°ì´í„°ì…‹\n",
        "- **Oxford-IIIT Pet**: ~7,400ì¥, 37 breeds, **trimap segmentation mask** í¬í•¨\n",
        "- 3-class segmentation: foreground (ë™ë¬¼) / background (ë°°ê²½) / boundary (ê²½ê³„)\n",
        "- torchvision ë‚´ì¥ ë°ì´í„°ì…‹ìœ¼ë¡œ ë³„ë„ ë‹¤ìš´ë¡œë“œ ë¶ˆí•„ìš”\n",
        "\n",
        "## ëª¨ë¸\n",
        "- **DeepLabV3** + **MobileNetV3-Large** backbone (torchvision pretrained)\n",
        "- classifier headë§Œ 3-classë¡œ êµì²´í•˜ì—¬ fine-tune\n",
        "- ~11M íŒŒë¼ë¯¸í„° â€” Colab T4ì—ì„œ ë¹ ë¥¸ í•™ìŠµ, Streamlit Cloud(CPU)ì—ì„œ ë¹ ë¥¸ ì¶”ë¡ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLVMsl0QA8Pf",
        "outputId": "756a1f9e-9553-4bc3-83c7-d2c01c4ab06f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.segmentation import deeplabv3_mobilenet_v3_large, DeepLabV3_MobileNet_V3_Large_Weights\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "\n",
        "CONFIG = {\n",
        "    \"batch_size\": 16,\n",
        "    \"lr\": 1e-3,\n",
        "    \"epochs\": 10,\n",
        "    \"num_classes\": 3,\n",
        "    \"img_size\": 128,\n",
        "    \"model_name\": \"deeplabv3_mobilenetv3_large\",\n",
        "    \"dataset\": \"oxford-iiit-pet\",\n",
        "    \"optimizer\": \"adam\",\n",
        "}\n",
        "\n",
        "SEG_CLASSES = {0: \"foreground\", 1: \"background\", 2: \"boundary\"}\n",
        "\n",
        "CLASS_LABELS = {i: name for i, name in SEG_CLASSES.items()}\n",
        "\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBIUwXJuA8Pf",
        "outputId": "d2ac7129-c929-4131-d4db-8c589a1cc291"
      },
      "source": [
        "# === Oxford-IIIT Pet ë°ì´í„° ë¡œë“œ + Custom Dataset ===\n",
        "\n",
        "raw_trainval = torchvision.datasets.OxfordIIITPet(\n",
        "    root=\"./data\", split=\"trainval\", target_types=\"segmentation\", download=True\n",
        ")\n",
        "raw_test = torchvision.datasets.OxfordIIITPet(\n",
        "    root=\"./data\", split=\"test\", target_types=\"segmentation\", download=True\n",
        ")\n",
        "\n",
        "\n",
        "class PetSegDataset(Dataset):\n",
        "    \"\"\"Oxford-IIIT Pet segmentation ë°ì´í„°ì…‹ ë˜í¼.\n",
        "\n",
        "    trimap ê°’(1, 2, 3)ì„ 0-indexed(0, 1, 2)ë¡œ ë³€í™˜í•˜ê³ ,\n",
        "    ì´ë¯¸ì§€ì™€ ë§ˆìŠ¤í¬ì— ë™ì¼í•œ Resizeë¥¼ ì ìš©í•œë‹¤.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_dataset, img_size, augment=False):\n",
        "        self.base = base_dataset\n",
        "        self.img_size = img_size\n",
        "        self.augment = augment\n",
        "        self.img_transform = T.Compose([\n",
        "            T.Resize((img_size, img_size)),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "        ])\n",
        "        self.mask_resize = T.Resize(\n",
        "            (img_size, img_size), interpolation=T.InterpolationMode.NEAREST\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, mask = self.base[idx]\n",
        "\n",
        "        if self.augment and random.random() > 0.5:\n",
        "            image = T.functional.hflip(image)\n",
        "            mask = T.functional.hflip(mask)\n",
        "\n",
        "        img_tensor = self.img_transform(image)\n",
        "        mask_tensor = self.mask_resize(mask)\n",
        "        mask_np = np.array(mask_tensor)\n",
        "        # trimap: 1=foreground, 2=background, 3=boundary â†’ 0-indexed\n",
        "        mask_np = mask_np - 1\n",
        "        mask_np = np.clip(mask_np, 0, 2)\n",
        "        mask_tensor = torch.from_numpy(mask_np).long().squeeze(0)\n",
        "\n",
        "        return img_tensor, mask_tensor\n",
        "\n",
        "\n",
        "trainset = PetSegDataset(raw_trainval, CONFIG[\"img_size\"], augment=True)\n",
        "testset = PetSegDataset(raw_test, CONFIG[\"img_size\"], augment=False)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=2)\n",
        "testloader = DataLoader(testset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Train: {len(trainset)}ì¥, Test: {len(testset)}ì¥\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792M/792M [00:28<00:00, 28.1MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19.2M/19.2M [00:01<00:00, 13.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 3680ì¥, Test: 3669ì¥\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "wEn9dWlAA8Pf",
        "outputId": "76b98b24-5b3f-44c8-c247-3c871133bb05"
      },
      "source": [
        "# === ë°ì´í„°ì…‹ Artifact ìƒì„± ===\n",
        "\n",
        "run = wandb.init(\n",
        "    entity=WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    config=CONFIG,\n",
        "    job_type=\"data-versioning\",\n",
        "    name=\"oxford-pet-data-versioning\",\n",
        ")\n",
        "\n",
        "artifact = wandb.Artifact(\n",
        "    \"oxford-iiit-pet\",\n",
        "    type=\"dataset\",\n",
        "    description=\"Oxford-IIIT Pet dataset with trimap segmentation masks\",\n",
        "    metadata={\n",
        "        \"num_train\": len(trainset),\n",
        "        \"num_test\": len(testset),\n",
        "        \"num_classes\": 3,\n",
        "        \"classes\": list(SEG_CLASSES.values()),\n",
        "        \"image_size\": f\"{CONFIG['img_size']}x{CONFIG['img_size']}\",\n",
        "        \"source\": \"torchvision.datasets.OxfordIIITPet\",\n",
        "    },\n",
        ")\n",
        "artifact.add_dir(\"./data/oxford-iiit-pet\")\n",
        "run.log_artifact(artifact)\n",
        "print(\"ë°ì´í„°ì…‹ Artifact ë¡œê¹… ì™„ë£Œ!\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.25.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260220_074824-bwtfwd7w</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/wandb-korea/e2e-demo-image-classification/runs/bwtfwd7w' target=\"_blank\">oxford-pet-data-versioning</a></strong> to <a href='https://wandb.ai/wandb-korea/e2e-demo-image-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/wandb-korea/e2e-demo-image-classification' target=\"_blank\">https://wandb.ai/wandb-korea/e2e-demo-image-classification</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/wandb-korea/e2e-demo-image-classification/runs/bwtfwd7w' target=\"_blank\">https://wandb.ai/wandb-korea/e2e-demo-image-classification/runs/bwtfwd7w</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (data/oxford-iiit-pet)... Done. 22.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë°ì´í„°ì…‹ Artifact ë¡œê¹… ì™„ë£Œ!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "eIQnoxzmA8Pf",
        "outputId": "7f077f2a-8341-447f-d0f1-2fda5cfa7535"
      },
      "source": [
        "# === ìƒ˜í”Œ ì´ë¯¸ì§€+ë§ˆìŠ¤í¬ ì‹œê°í™” (wandb.Table + ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´) ===\n",
        "\n",
        "def get_raw_image_and_mask(dataset, idx, img_size):\n",
        "    \"\"\"ì‹œê°í™”ìš© ì›ë³¸(ì •ê·œí™” ì•ˆ ëœ) ì´ë¯¸ì§€ì™€ ë§ˆìŠ¤í¬ë¥¼ ë°˜í™˜í•œë‹¤.\"\"\"\n",
        "    image, mask = dataset.base[idx]\n",
        "    image = image.resize((img_size, img_size))\n",
        "    mask = mask.resize((img_size, img_size), Image.NEAREST)\n",
        "    mask_np = np.array(mask) - 1\n",
        "    mask_np = np.clip(mask_np, 0, 2)\n",
        "    return image, mask_np\n",
        "\n",
        "\n",
        "table = wandb.Table(columns=[\"Image with Mask\", \"Image\", \"Mask\", \"Breed\"])\n",
        "\n",
        "indices = random.sample(range(len(trainset)), 20)\n",
        "for idx in indices:\n",
        "    image, mask_np = get_raw_image_and_mask(trainset, idx, CONFIG[\"img_size\"])\n",
        "\n",
        "    img_with_mask = wandb.Image(\n",
        "        image,\n",
        "        masks={\n",
        "            \"ground_truth\": {\n",
        "                \"mask_data\": mask_np,\n",
        "                \"class_labels\": CLASS_LABELS,\n",
        "            }\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # ë§ˆìŠ¤í¬ë¥¼ ì»¬ëŸ¬ë§µìœ¼ë¡œ ë³€í™˜ (foreground=ì´ˆë¡, background=ê²€ì •, boundary=ë¹¨ê°•)\n",
        "    color_mask = np.zeros((*mask_np.shape, 3), dtype=np.uint8)\n",
        "    color_mask[mask_np == 0] = [0, 200, 0]\n",
        "    color_mask[mask_np == 1] = [40, 40, 40]\n",
        "    color_mask[mask_np == 2] = [200, 0, 0]\n",
        "\n",
        "    # breed = raw_trainval._images[idx].split(\"_\")[:-1]\n",
        "    breed = raw_trainval._images[idx].stem.split(\"_\")[:-1]\n",
        "    breed_name = \" \".join(breed)\n",
        "\n",
        "    table.add_data(\n",
        "        img_with_mask,\n",
        "        wandb.Image(image),\n",
        "        wandb.Image(Image.fromarray(color_mask)),\n",
        "        breed_name,\n",
        "    )\n",
        "\n",
        "wandb.log({\"dataset_preview\": table})\n",
        "wandb.finish()\n",
        "print(\"ë°ì´í„°ì…‹ ë¯¸ë¦¬ë³´ê¸° í…Œì´ë¸” ë¡œê¹… ì™„ë£Œ!\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'PosixPath' object has no attribute 'split'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3133775100.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mcolor_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_np\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mbreed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_trainval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mbreed_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbreed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute 'split'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJc7pgPtA8Pf"
      },
      "source": [
        "# === ëª¨ë¸ ì •ì˜ (DeepLabV3 + MobileNetV3-Large) ===\n",
        "\n",
        "def create_model(num_classes=3):\n",
        "    \"\"\"DeepLabV3 + MobileNetV3-Large backbone.\n",
        "\n",
        "    pretrained backbone ìœ„ì— classifier headë§Œ num_classesë¡œ êµì²´í•œë‹¤.\n",
        "    ~11M íŒŒë¼ë¯¸í„°ë¡œ Colab T4 í•™ìŠµ + Streamlit Cloud(CPU) ì¶”ë¡ ì— ì í•©í•˜ë‹¤.\n",
        "    \"\"\"\n",
        "    model = deeplabv3_mobilenet_v3_large(\n",
        "        weights=DeepLabV3_MobileNet_V3_Large_Weights.DEFAULT\n",
        "    )\n",
        "    model.classifier[-1] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "    model.aux_classifier[-1] = nn.Conv2d(10, num_classes, kernel_size=1)\n",
        "    return model\n",
        "\n",
        "\n",
        "model = create_model(CONFIG[\"num_classes\"]).to(device)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1jrpBxcA8Pg"
      },
      "source": [
        "# === í•™ìŠµ ì „ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ ë“±ë¡ ===\n",
        "\n",
        "run = wandb.init(\n",
        "    entity=WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    config=CONFIG,\n",
        "    job_type=\"model-baseline\",\n",
        "    name=\"deeplabv3-pet-untrained\",\n",
        ")\n",
        "\n",
        "data_artifact = run.use_artifact(f\"{WANDB_ENTITY}/{WANDB_PROJECT}/oxford-iiit-pet:latest\")\n",
        "\n",
        "# í•™ìŠµ ì „ ëª¨ë¸ Mean IoU ì¸¡ì •\n",
        "model.eval()\n",
        "iou_sum, pixel_correct, pixel_total, num_batches = 0.0, 0, 0, 0\n",
        "with torch.no_grad():\n",
        "    for images, masks in testloader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        outputs = model(images)[\"out\"]\n",
        "        preds = outputs.argmax(1)\n",
        "        pixel_correct += (preds == masks).sum().item()\n",
        "        pixel_total += masks.numel()\n",
        "        num_batches += 1\n",
        "        if num_batches >= 10:\n",
        "            break\n",
        "\n",
        "baseline_pixel_acc = pixel_correct / pixel_total\n",
        "print(f\"í•™ìŠµ ì „ ë² ì´ìŠ¤ë¼ì¸ Pixel Accuracy: {baseline_pixel_acc:.4f}\")\n",
        "\n",
        "wandb.log({\"val/pixel_acc\": baseline_pixel_acc})\n",
        "wandb.summary[\"best_mean_iou\"] = 0.0\n",
        "\n",
        "torch.save(model.state_dict(), \"untrained_model.pth\")\n",
        "\n",
        "baseline_artifact = wandb.Artifact(\n",
        "    \"deeplabv3-pet-seg\",\n",
        "    type=\"model\",\n",
        "    description=\"DeepLabV3 MobileNetV3-Large untrained baseline\",\n",
        "    metadata={\n",
        "        \"model_type\": \"segmentation\",\n",
        "        \"model_architecture\": \"deeplabv3_mobilenetv3_large\",\n",
        "        \"dataset\": \"oxford-iiit-pet\",\n",
        "        \"num_classes\": 3,\n",
        "        \"best_mean_iou\": 0.0,\n",
        "        \"classes\": list(SEG_CLASSES.values()),\n",
        "        \"framework\": \"pytorch\",\n",
        "        \"input_size\": [3, CONFIG[\"img_size\"], CONFIG[\"img_size\"]],\n",
        "        \"trained\": False,\n",
        "    },\n",
        ")\n",
        "baseline_artifact.add_file(\"untrained_model.pth\", name=\"model.pth\")\n",
        "run.log_artifact(baseline_artifact)\n",
        "\n",
        "run.link_artifact(\n",
        "    baseline_artifact,\n",
        "    f\"{WANDB_REGISTRY_NAME}/pet-segmenter\",\n",
        "    aliases=[\"baseline\"],\n",
        ")\n",
        "print(\"í•™ìŠµ ì „ ëª¨ë¸ì„ Registryì— 'baseline' aliasë¡œ ë“±ë¡ ì™„ë£Œ!\")\n",
        "\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rliYAOEA8Pg"
      },
      "source": [
        "# === í•™ìŠµ ë£¨í”„ ===\n",
        "\n",
        "run = wandb.init(\n",
        "    entity=WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    config=CONFIG,\n",
        "    job_type=\"training\",\n",
        "    name=\"deeplabv3-pet-baseline\",\n",
        ")\n",
        "\n",
        "data_artifact = run.use_artifact(f\"{WANDB_ENTITY}/{WANDB_PROJECT}/oxford-iiit-pet:latest\")\n",
        "print(f\"ë°ì´í„° Artifact ì‚¬ìš©: {data_artifact.name}:{data_artifact.version}\")\n",
        "\n",
        "\n",
        "# --- ë©”íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜ ---\n",
        "\n",
        "def compute_iou(pred, target, num_classes):\n",
        "    \"\"\"í´ë˜ìŠ¤ë³„ IoUë¥¼ ê³„ì‚°í•˜ê³  í‰ê· (mIoU)ì„ ë°˜í™˜í•œë‹¤.\"\"\"\n",
        "    ious = []\n",
        "    for cls in range(num_classes):\n",
        "        pred_cls = (pred == cls)\n",
        "        target_cls = (target == cls)\n",
        "        intersection = (pred_cls & target_cls).sum().item()\n",
        "        union = (pred_cls | target_cls).sum().item()\n",
        "        if union == 0:\n",
        "            continue\n",
        "        ious.append(intersection / union)\n",
        "    return np.mean(ious) if ious else 0.0\n",
        "\n",
        "\n",
        "def compute_pixel_accuracy(pred, target):\n",
        "    return (pred == target).sum().item() / target.numel()\n",
        "\n",
        "\n",
        "def compute_dice(pred, target, num_classes):\n",
        "    \"\"\"í´ë˜ìŠ¤ë³„ Dice coefficientë¥¼ ê³„ì‚°í•˜ê³  í‰ê· ì„ ë°˜í™˜í•œë‹¤.\"\"\"\n",
        "    dices = []\n",
        "    for cls in range(num_classes):\n",
        "        pred_cls = (pred == cls)\n",
        "        target_cls = (target == cls)\n",
        "        intersection = (pred_cls & target_cls).sum().item()\n",
        "        total = pred_cls.sum().item() + target_cls.sum().item()\n",
        "        if total == 0:\n",
        "            continue\n",
        "        dices.append(2.0 * intersection / total)\n",
        "    return np.mean(dices) if dices else 0.0\n",
        "\n",
        "\n",
        "# --- í•™ìŠµ/ê²€ì¦ í•¨ìˆ˜ ---\n",
        "\n",
        "def train_one_epoch(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss, pixel_correct, pixel_total = 0.0, 0, 0\n",
        "\n",
        "    for batch_idx, (images, masks) in enumerate(loader):\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)[\"out\"]\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        preds = outputs.argmax(1)\n",
        "        pixel_correct += (preds == masks).sum().item()\n",
        "        pixel_total += masks.numel()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            wandb.log({\n",
        "                \"train/batch_loss\": loss.item(),\n",
        "            })\n",
        "\n",
        "    return running_loss / len(loader), pixel_correct / pixel_total\n",
        "\n",
        "\n",
        "def evaluate(model, loader, criterion, num_classes):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    total_iou, total_pixel_acc, total_dice, num_batches = 0.0, 0.0, 0.0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            outputs = model(images)[\"out\"]\n",
        "            loss = criterion(outputs, masks)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            preds = outputs.argmax(1)\n",
        "            total_iou += compute_iou(preds, masks, num_classes)\n",
        "            total_pixel_acc += compute_pixel_accuracy(preds, masks)\n",
        "            total_dice += compute_dice(preds, masks, num_classes)\n",
        "            num_batches += 1\n",
        "\n",
        "    n = num_batches\n",
        "    return running_loss / len(loader), total_iou / n, total_pixel_acc / n, total_dice / n\n",
        "\n",
        "\n",
        "# --- í•™ìŠµ ì‹¤í–‰ ---\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "if CONFIG[\"optimizer\"] == \"adam\":\n",
        "    optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "elif CONFIG[\"optimizer\"] == \"sgd\":\n",
        "    optimizer = optim.SGD(model.parameters(), lr=CONFIG[\"lr\"], momentum=0.9, weight_decay=5e-4)\n",
        "elif CONFIG[\"optimizer\"] == \"adamw\":\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG[\"lr\"], weight_decay=1e-2)\n",
        "\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG[\"epochs\"])\n",
        "\n",
        "wandb.watch(model, criterion, log=\"all\", log_freq=100)\n",
        "\n",
        "best_iou = 0.0\n",
        "\n",
        "for epoch in range(CONFIG[\"epochs\"]):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_pixel_acc = train_one_epoch(model, trainloader, criterion, optimizer)\n",
        "    val_loss, val_iou, val_pixel_acc, val_dice = evaluate(\n",
        "        model, testloader, criterion, CONFIG[\"num_classes\"]\n",
        "    )\n",
        "    scheduler.step()\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train/loss\": train_loss,\n",
        "        \"train/pixel_acc\": train_pixel_acc,\n",
        "        \"val/loss\": val_loss,\n",
        "        \"val/mean_iou\": val_iou,\n",
        "        \"val/pixel_acc\": val_pixel_acc,\n",
        "        \"val/dice\": val_dice,\n",
        "        \"lr\": scheduler.get_last_lr()[0],\n",
        "        \"epoch_time_sec\": epoch_time,\n",
        "    })\n",
        "\n",
        "    # ë§¤ epoch ì˜ˆì¸¡ ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´ ë¡œê¹…\n",
        "    model.eval()\n",
        "    sample_images, sample_masks = next(iter(testloader))\n",
        "    with torch.no_grad():\n",
        "        sample_outputs = model(sample_images.to(device))[\"out\"]\n",
        "        sample_preds = sample_outputs.argmax(1).cpu().numpy()\n",
        "\n",
        "    mean = torch.tensor(IMAGENET_MEAN).view(3, 1, 1)\n",
        "    std = torch.tensor(IMAGENET_STD).view(3, 1, 1)\n",
        "\n",
        "    wandb_images = []\n",
        "    for i in range(min(6, len(sample_images))):\n",
        "        img = sample_images[i] * std + mean\n",
        "        img = img.clamp(0, 1)\n",
        "        img_pil = T.ToPILImage()(img)\n",
        "\n",
        "        wandb_images.append(wandb.Image(\n",
        "            img_pil,\n",
        "            masks={\n",
        "                \"predictions\": {\n",
        "                    \"mask_data\": sample_preds[i],\n",
        "                    \"class_labels\": CLASS_LABELS,\n",
        "                },\n",
        "                \"ground_truth\": {\n",
        "                    \"mask_data\": sample_masks[i].numpy(),\n",
        "                    \"class_labels\": CLASS_LABELS,\n",
        "                },\n",
        "            },\n",
        "        ))\n",
        "    wandb.log({\"predictions\": wandb_images})\n",
        "\n",
        "    if val_iou > best_iou:\n",
        "        best_iou = val_iou\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "    print(\n",
        "        f\"Epoch [{epoch+1}/{CONFIG['epochs']}] \"\n",
        "        f\"Train Loss: {train_loss:.4f} | \"\n",
        "        f\"Val Loss: {val_loss:.4f} mIoU: {val_iou:.4f} \"\n",
        "        f\"PixAcc: {val_pixel_acc:.4f} Dice: {val_dice:.4f}\"\n",
        "    )\n",
        "\n",
        "wandb.summary[\"best_mean_iou\"] = best_iou\n",
        "print(f\"\\ní•™ìŠµ ì™„ë£Œ! Best Val mIoU: {best_iou:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF3IXQ0FA8Pg"
      },
      "source": [
        "# === ê²€ì¦ ê²°ê³¼ wandb.Table ===\n",
        "\n",
        "model.load_state_dict(torch.load(\"best_model.pth\", map_location=device, weights_only=True))\n",
        "model.eval()\n",
        "\n",
        "results_table = wandb.Table(\n",
        "    columns=[\"Image\", \"Ground Truth\", \"Prediction\", \"Overlay\", \"mIoU\", \"Pixel Acc\", \"Dice\"]\n",
        ")\n",
        "\n",
        "mean = torch.tensor(IMAGENET_MEAN).view(3, 1, 1)\n",
        "std = torch.tensor(IMAGENET_STD).view(3, 1, 1)\n",
        "\n",
        "num_samples = 50\n",
        "indices = random.sample(range(len(testset)), num_samples)\n",
        "\n",
        "for idx in indices:\n",
        "    img_tensor, mask_tensor = testset[idx]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(img_tensor.unsqueeze(0).to(device))[\"out\"]\n",
        "        pred = output.argmax(1).squeeze(0).cpu()\n",
        "\n",
        "    img_denorm = img_tensor * std + mean\n",
        "    img_denorm = img_denorm.clamp(0, 1)\n",
        "    img_pil = T.ToPILImage()(img_denorm)\n",
        "\n",
        "    sample_iou = compute_iou(pred, mask_tensor, CONFIG[\"num_classes\"])\n",
        "    sample_pixel_acc = compute_pixel_accuracy(pred, mask_tensor)\n",
        "    sample_dice = compute_dice(pred, mask_tensor, CONFIG[\"num_classes\"])\n",
        "\n",
        "    overlay_img = wandb.Image(\n",
        "        img_pil,\n",
        "        masks={\n",
        "            \"prediction\": {\n",
        "                \"mask_data\": pred.numpy(),\n",
        "                \"class_labels\": CLASS_LABELS,\n",
        "            },\n",
        "            \"ground_truth\": {\n",
        "                \"mask_data\": mask_tensor.numpy(),\n",
        "                \"class_labels\": CLASS_LABELS,\n",
        "            },\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # GT/Pred ë§ˆìŠ¤í¬ ì»¬ëŸ¬ë§µ\n",
        "    def mask_to_color(m):\n",
        "        c = np.zeros((*m.shape, 3), dtype=np.uint8)\n",
        "        c[m == 0] = [0, 200, 0]\n",
        "        c[m == 1] = [40, 40, 40]\n",
        "        c[m == 2] = [200, 0, 0]\n",
        "        return Image.fromarray(c)\n",
        "\n",
        "    results_table.add_data(\n",
        "        wandb.Image(img_pil),\n",
        "        wandb.Image(mask_to_color(mask_tensor.numpy())),\n",
        "        wandb.Image(mask_to_color(pred.numpy())),\n",
        "        overlay_img,\n",
        "        round(sample_iou, 4),\n",
        "        round(sample_pixel_acc, 4),\n",
        "        round(sample_dice, 4),\n",
        "    )\n",
        "\n",
        "wandb.log({\"test_predictions\": results_table})\n",
        "print(f\"í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ê²°ê³¼ {num_samples}ê±´ ë¡œê¹… ì™„ë£Œ!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yUARj_yA8Pg"
      },
      "source": [
        "# === ëª¨ë¸ Artifact ì €ì¥ ===\n",
        "\n",
        "model_artifact = wandb.Artifact(\n",
        "    \"deeplabv3-pet-seg\",\n",
        "    type=\"model\",\n",
        "    description=\"DeepLabV3 MobileNetV3-Large fine-tuned on Oxford-IIIT Pet\",\n",
        "    metadata={\n",
        "        \"model_type\": \"segmentation\",\n",
        "        \"model_architecture\": \"deeplabv3_mobilenetv3_large\",\n",
        "        \"dataset\": \"oxford-iiit-pet\",\n",
        "        \"num_classes\": 3,\n",
        "        \"best_mean_iou\": best_iou,\n",
        "        \"classes\": list(SEG_CLASSES.values()),\n",
        "        \"framework\": \"pytorch\",\n",
        "        \"input_size\": [3, CONFIG[\"img_size\"], CONFIG[\"img_size\"]],\n",
        "    },\n",
        ")\n",
        "model_artifact.add_file(\"best_model.pth\", name=\"model.pth\")\n",
        "run.log_artifact(model_artifact)\n",
        "print(\"ëª¨ë¸ Artifact ë¡œê¹… ì™„ë£Œ!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFsNZMzZA8Pg"
      },
      "source": [
        "# === Model Registry ë“±ë¡ ===\n",
        "\n",
        "run.link_artifact(\n",
        "    model_artifact,\n",
        "    f\"{WANDB_REGISTRY_NAME}/pet-segmenter\",\n",
        "    aliases=[\"staging\"],\n",
        ")\n",
        "print(\"Model Registryì— 'staging' aliasë¡œ ë“±ë¡ ì™„ë£Œ!\")\n",
        "\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJVDcL01A8Pg"
      },
      "source": [
        "## Hyperparameter Sweep\n",
        "\n",
        "**Bayesian ìµœì í™”**ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì„ íƒìƒ‰í•©ë‹ˆë‹¤.\n",
        "\n",
        "| íŒŒë¼ë¯¸í„° | íƒìƒ‰ ë²”ìœ„ |\n",
        "|-----------|------------|\n",
        "| Learning Rate | 1e-5 ~ 1e-2 (log uniform) |\n",
        "| Image Size | 128, 256 |\n",
        "| Freeze Backbone | True, False |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LrVaZbtA8Pg"
      },
      "source": [
        "# === Sweep ì„¤ì • ë° ì‹¤í–‰ ===\n",
        "\n",
        "sweep_config = {\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\"name\": \"val/mean_iou\", \"goal\": \"maximize\"},\n",
        "    \"parameters\": {\n",
        "        \"lr\": {\"min\": 1e-5, \"max\": 1e-2, \"distribution\": \"log_uniform_values\"},\n",
        "        \"img_size\": {\"values\": [128, 256]},\n",
        "        \"freeze_backbone\": {\"values\": [True, False]},\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "def sweep_train():\n",
        "    \"\"\"Sweep í•™ìŠµ í•¨ìˆ˜\"\"\"\n",
        "    run = wandb.init(entity=WANDB_ENTITY, config=CONFIG)\n",
        "    config = wandb.config\n",
        "\n",
        "    sweep_img_size = config.img_size\n",
        "    sweep_trainset = PetSegDataset(raw_trainval, sweep_img_size, augment=True)\n",
        "    sweep_testset = PetSegDataset(raw_test, sweep_img_size, augment=False)\n",
        "    sweep_trainloader = DataLoader(\n",
        "        sweep_trainset, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=2\n",
        "    )\n",
        "    sweep_testloader = DataLoader(\n",
        "        sweep_testset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=2\n",
        "    )\n",
        "\n",
        "    sweep_model = create_model(CONFIG[\"num_classes\"]).to(device)\n",
        "\n",
        "    if config.freeze_backbone:\n",
        "        for param in sweep_model.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    trainable = [p for p in sweep_model.parameters() if p.requires_grad]\n",
        "    opt = optim.Adam(trainable, lr=config.lr)\n",
        "\n",
        "    for epoch in range(3):\n",
        "        train_loss, _ = train_one_epoch(sweep_model, sweep_trainloader, criterion, opt)\n",
        "        val_loss, val_iou, val_pixel_acc, val_dice = evaluate(\n",
        "            sweep_model, sweep_testloader, criterion, CONFIG[\"num_classes\"]\n",
        "        )\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train/loss\": train_loss,\n",
        "            \"val/loss\": val_loss,\n",
        "            \"val/mean_iou\": val_iou,\n",
        "            \"val/pixel_acc\": val_pixel_acc,\n",
        "            \"val/dice\": val_dice,\n",
        "        })\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, entity=WANDB_ENTITY, project=WANDB_PROJECT)\n",
        "wandb.agent(sweep_id, function=sweep_train, count=5)\n",
        "print(\"Sweep ì™„ë£Œ!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1XuBeSHA8Pg"
      },
      "source": [
        "# === Report ìƒì„± ===\n",
        "\n",
        "import wandb_workspaces.reports.v2 as wr\n",
        "\n",
        "report = wr.Report(\n",
        "    entity=WANDB_ENTITY,\n",
        "    project=WANDB_PROJECT,\n",
        "    title=\"Oxford-IIIT Pet Segmentation â€” ì‹¤í—˜ ê²°ê³¼ ë¦¬í¬íŠ¸\",\n",
        "    description=\"DeepLabV3 MobileNetV3-Large segmentation ì‹¤í—˜ ê²°ê³¼ ë° Sweep ë¶„ì„\",\n",
        ")\n",
        "\n",
        "report.blocks = [\n",
        "    wr.TableOfContents(),\n",
        "\n",
        "    wr.H1(\"1. ì‹¤í—˜ ê°œìš”\"),\n",
        "    wr.P(\n",
        "        \"Oxford-IIIT Pet ë°ì´í„°ì…‹ì— ëŒ€í•œ DeepLabV3 ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í—˜ ê²°ê³¼ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤. \"\n",
        "        \"W&Bì˜ Experiment Tracking, Artifacts, Sweeps, Model Registry ê¸°ëŠ¥ì„ í™œìš©í•˜ì˜€ìŠµë‹ˆë‹¤.\"\n",
        "    ),\n",
        "\n",
        "    wr.H1(\"2. í•™ìŠµ ê²°ê³¼\"),\n",
        "    wr.PanelGrid(\n",
        "        runsets=[\n",
        "            wr.Runset(entity=WANDB_ENTITY, project=WANDB_PROJECT)\n",
        "        ],\n",
        "        panels=[\n",
        "            wr.LinePlot(title=\"Training Loss\", x=\"epoch\", y=[\"train/loss\"]),\n",
        "            wr.LinePlot(title=\"Validation Loss\", x=\"epoch\", y=[\"val/loss\"]),\n",
        "            wr.LinePlot(title=\"Mean IoU\", x=\"epoch\", y=[\"val/mean_iou\"]),\n",
        "            wr.LinePlot(title=\"Pixel Accuracy\", x=\"epoch\", y=[\"val/pixel_acc\"]),\n",
        "            wr.LinePlot(title=\"Dice Coefficient\", x=\"epoch\", y=[\"val/dice\"]),\n",
        "        ],\n",
        "    ),\n",
        "\n",
        "    wr.H1(\"3. Sweep ë¶„ì„\"),\n",
        "    wr.P(\"Bayesian ìµœì í™”ë¥¼ í†µí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ê²°ê³¼:\"),\n",
        "    wr.PanelGrid(\n",
        "        runsets=[\n",
        "            wr.Runset(entity=WANDB_ENTITY, project=WANDB_PROJECT)\n",
        "        ],\n",
        "        panels=[\n",
        "            wr.ParallelCoordinatesPlot(\n",
        "                columns=[\n",
        "                    wr.ParallelCoordinatesPlotColumn(metric=\"c::lr\"),\n",
        "                    wr.ParallelCoordinatesPlotColumn(metric=\"c::img_size\"),\n",
        "                    wr.ParallelCoordinatesPlotColumn(metric=\"c::freeze_backbone\"),\n",
        "                    wr.ParallelCoordinatesPlotColumn(metric=\"val/mean_iou\"),\n",
        "                ],\n",
        "            ),\n",
        "            wr.ScalarChart(title=\"Best Mean IoU\", metric=\"val/mean_iou\"),\n",
        "            wr.BarPlot(title=\"Mean IoU by Run\", metrics=[\"val/mean_iou\"]),\n",
        "        ],\n",
        "    ),\n",
        "\n",
        "    wr.H1(\"4. ë‹¤ìŒ ë‹¨ê³„\"),\n",
        "    wr.P(\n",
        "        \"ìµœì  ëª¨ë¸ì„ Model Registryì˜ 'production' aliasë¡œ ìŠ¹ê²©í•˜ì—¬ \"\n",
        "        \"Streamlit Cloudì—ì„œ ì‹¤ì‹œê°„ ì¶”ë¡  ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "report.save()\n",
        "print(f\"Report ìƒì„± ì™„ë£Œ! URL: {report.url}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM30MiqdA8Ph"
      },
      "source": [
        "wandb.finish()\n",
        "print(\"\\ní•™ìŠµ & ì‹¤í—˜ ì¶”ì  ì™„ë£Œ!\")\n",
        "print(\"W&B ëŒ€ì‹œë³´ë“œì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}